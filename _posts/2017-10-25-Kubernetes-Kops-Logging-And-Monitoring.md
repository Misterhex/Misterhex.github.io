## TL;DR 
Follow this [example github repo](https://github.com/Misterhex/kubernetes-logging-monitoring-example) to setup kubernetes using kops, then run daemonsets for fluentd and metricbeat for monitoring.

## Background
Recently, i am helping our client to move applications to containerized microservices and architecture.

Moving to `docker` containers is great, but docker alone does not allow us to orchestrate our containers in productions and making sure that they can scale and provide highly availablility.

In this post, i would like to document how we are running `kubernetes` in production.

## KOPS ( Kubernetes Operation)

We adopted `kubernetes` as our containers orchestrator of choice. There are various ways to operate a kubernetes clusters and it can range from being really complex to simple hosted cloud solutions.

These are some of the various tools and platforms:

- Kops ( run on aws )
- Google container engine ( google cloud )
- Jujucharms, canonical distribution of kubernetes ( baremetal, metal cloud ( Machine as a service ), virtual machines )
- Kubeadm ( baremetal, virtual machines )
- More..

We are using `kops` to provision our cluster `aws`, using `kops` we get multi-az nodes deployment and auto scaling group at the cluster level. We actually evaluated a couple of options before deciding on `kops`. Our client is already on `aws` and is comfortable with it. also, `kops` project has quite a strong community involvement.

Learn more about `kops` [here](https://github.com/kubernetes/kops)

## Kubernetes Dashboard

Running kubernetes dashboard allow us to visualize what kubernetes resources are running or getting scheduled.

![cname file content]({{ site.url }}/assets/img/kube-dashboard.png)

## Datastore

We run `elasticsearch` outside of kubernetes to store log data generated by fluentd and metricbeat daemonsets. Running ELK outside kubernetes is more robust, in case kubernetes clusters goes down, we can still see logs on what happened before the kubernetes died.

## Centralized logging

Another problems to solve before going into production is to have centralized logging pods/containers running in kubernetes. Pods/containers are running in different kubernetes nodes, scaling up and down, it is hard to understand what is going on without centralized logging. What we want is a single place to look logs from stdout and stderr streams of running containers to quickly detect and diagnose issues.

We run `fluentd` as daemonsets on every kubernetes nodes. In kubernetes, all containers write their stdout/stderr streams to the `/var/log/containers/` folders. Fluentd can be configured to monitor this folder and then send logs into elasticsearch.

![cname file content]({{ site.url }}/assets/img/centralized_logging.png)

## Monitoring

Monitoring is done using [metricbeat](https://www.elastic.co/products/beats/metricbeat) and [kube-state-metric](https://github.com/kubernetes/kube-state-metrics). `kube-state-metric` is a simple service that listens to the Kubernetes API server and generates metrics about the state of the objects. `Metricbeat` grab data from `kube-state-metric` into `elasticsearch`.

Kubernetes dashboard in kibana:
![cname file content]({{ site.url }}/assets/img/k8_1.png)

Metricbeat dashboard in kibana:
![cname file content]({{ site.url }}/assets/img/k8_2.png)

## Conclusion

Centralized logging and monitoring is important to setup before moving to production with kubernetes. I have created an [example kubernetes github repo](https://github.com/Misterhex/kubernetes-logging-monitoring-example) that contains the kubernetes yaml files to deploy everything we discussed.