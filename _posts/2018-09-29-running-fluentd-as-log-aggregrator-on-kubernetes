---
title:  "Running fluentd as a log aggregrator on kubernetes"
tags: [devops, fluentd, kubernetes]
---

Recently, one of my task at work was setting up log aggregration using fluentd on kubernetes. 

Our fluentd serve as the unified logging layer for our platform, we ingest logs from various inputs and send copies of the data out to multiple elasticsearch clusters.

## Setup
- Using out_forward plugin, we ship all docker containers logs via fluentbit.
- Using in_systemd plugin, we collect systemd/journald logs from control planes such as kube api server logs, kubelet logs, docker daemon, auditd, etcd, etc.
- Using in_http plugin, we point our kong's api gateway http logs to fluentd.
- Various sidecar fluentbit containers that tail, grep and parse logs from application containers, then shipping them with out_forward to fluentd.
- Counter for incoming logs with fluentd_prometheus plugin, and our fluentd expose these as prometheus's metrics together for observability.
- Run fluentd with horizontal pod autoscaler for autoscaling.
7) Proper readiness check for robust rolling update.
8) Using the fluentd-elasticsearch plugin to send copies of logs to multiple elasticsearch clusters



## Advantages
These are the advantages of our setup:

- Very easily configure new input and output source.
- Scale horizontally by having multiple pods and load balance them.
- Monitor all fluentd pods with prometheus plugin, to fire off alerts when we have buffer/back pressure building up, or simply when our output sources is down.
- Using horizontalpod autoscaler, we can dynamically scale fluentd to handle spikes, e.g. a container that is on a crash loop emittings stacktrace.
- Fluentd is a reliable message forwarder with at-least-once message delivery semantics for critical audit logs by having buffering mechanism in place, buffering can be backed by memory or disk.
- Nice organization of configuration using @LABEL and @INCLUDE. We also have our own mustache templating for more complex configuration.
- Rolling out new version using rolling update.


## Fluentd vs Logstash
We prefer fluentd over logstash as it is part of the cncf projects and also because it support shipping logs to multiple sinks rather than just elasticsearch. Read more about unified logging layer here.

## Tips
- Remember to set `<store ignore_error>` if you have multiple es clusters. [https://docs.fluentd.org/v1.0/articles/out_copy#<store>-section](https://docs.fluentd.org/v1.0/articles/out_copy#<store>-section)
-  set `RUBY_GC_HEAP_OLDOBJECT_LIMIT_FACTOR=0.9` when running in a pod with lower memory limit to prevent getting OOM killed by kubernetes.



